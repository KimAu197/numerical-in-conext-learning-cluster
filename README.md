# Numerical In-Context Learning for Clustering with Linear Transformers

This repository contains code developed for my undergraduate thesis on **in-context learning with linear Transformers** applied to **numerical clustering tasks**.  
The experiments investigate whether linear Transformers generalize clustering rules, or instead behave as **memory-driven template matchers**.

## ğŸ“‚ Repository Structure

- **environment.yml** â€“ dependencies and environment setup  
- **data.json**, **all_results_o2.json** â€“ example datasets / experiment outputs  
- **non-linear-regression-IVP.ipynb**, **cluster.ipynb** â€“ Jupyter notebooks for exploration and visualization  

### `src/`
- **train.py** â€“ training entry point  
- **eval_cluster.py** â€“ evaluation of clustering tasks  
- **graph.py** â€“ synthetic data generation / sampling  
- **models.py**, **base_models.py** â€“ model definitions (linear Transformer, etc.)  
- **curriculum.py**, **samplers.py**, **tasks.py** â€“ task and data curriculum utilities  
- **plot_utils.py** â€“ helper functions for plotting results  
- **conf/** â€“ YAML configs for models and tasks  

---

## âš™ï¸ Setup

Create the conda environment from `environment.yml`:

```bash
conda env create -f environment.yml
conda activate num-cluster
```

## ğŸš€ Usage

1. Training

Run training on clustering tasks:

```bash
python src/train.py --config src/conf/cluster_2d.yaml # change xd for different dimention
```

2. Evaluation

Evaluate a trained model:

```bash
python src/eval_cluster.py --config src/conf/cluster_2d_eval.yaml



## ğŸ“Š Results
	â€¢	Models perform well at template matching (memorizing cluster examples seen in context).
	â€¢	Generalization to new clustering rules or shapes is limited.
	â€¢	Behavior suggests linear Transformers act more like memory-based pattern matchers rather than learners of universal clustering functions.

